{
  "nbformat_minor": 1, 
  "nbformat": 4, 
  "cells": [
    {
      "source": [
        "from __future__ import absolute_import\n", 
        "from __future__ import division\n", 
        "from __future__ import print_function\n", 
        "\n", 
        "import os\n", 
        "import time\n", 
        "\n", 
        "import tensorflow as tf\n", 
        "\n", 
        "from dataloader import DataLoader\n", 
        "from model import Network\n", 
        "from utils import pre_process, validate\n", 
        "from utils import print_configuration_op\n", 
        "\n", 
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n", 
        "\n", 
        "# 0 = all messages are logged (default behavior)\n", 
        "# 1 = INFO messages are not printed\n", 
        "# 2 = INFO and WARNING messages are not printed\n", 
        "# 3 = INFO, WARNING, and ERROR messages are not printed\n", 
        "\n", 
        "Flags = tf.app.flags\n", 
        "\n", 
        "# The system parameter\n", 
        "Flags.DEFINE_string('output_dir', '/handwritten-data/experiment_sign',\n", 
        "                    'The output directory of the checkpoint')\n", 
        "Flags.DEFINE_string('summary_dir', '/handwritten-data/experiment_sign/log/',\n", 
        "                    'The dirctory to output the summary')\n", 
        "Flags.DEFINE_string('mode', 'train', 'The mode of the model train, test.')\n", 
        "Flags.DEFINE_string('checkpoint', None, 'If provided, the weight will be restored from the provided checkpoint.'\n", 
        "                                        'Checkpoint folder (Latest checkpoint will be taken)')\n", 
        "Flags.DEFINE_boolean('pre_trained_model', False,\n", 
        "                     'If set True, the weight will be loaded but the global_step will still '\n", 
        "                     'be 0. If set False, you are going to continue the training. That is, '\n", 
        "                     'the global_step will be initialized from the checkpoint, too')\n", 
        "\n", 
        "# DataLoader Parameters\n", 
        "Flags.DEFINE_string('train_dir', '/handwritten-data/signatures/full_org',\n", 
        "                    'The train data directory')\n", 
        "Flags.DEFINE_string('val_dir',\n", 
        "                    '/SigComp2009-training/NISDCC-offline-all-001-051-6g',\n", 
        "                    'The validation data directory')\n", 
        "Flags.DEFINE_string('train_dataset_name', 'kaggle_signature', 'https://cedar.buffalo.edu/NIJ/data/signatures.rar')\n", 
        "Flags.DEFINE_string('val_dataset_name', 'kaggle_signature', '')\n", 
        "Flags.DEFINE_integer('batch_labels_size', 16, 'Number of labels in each batch. min 2, P')\n", 
        "Flags.DEFINE_integer('batch_image_per_label', 4, 'Number of images per label. min 2, K, batch size = P*K')\n", 
        "Flags.DEFINE_integer('val_batch_image_per_label', 5, 'Number of images per label for validation.')\n", 
        "Flags.DEFINE_integer('val_enrollment_size', 5, 'Number of images per label for enrollment size.')\n", 
        "Flags.DEFINE_integer('batch_thread', 4, 'The number of threads to process image queue for generating batches')\n", 
        "Flags.DEFINE_integer('image_size', 224, 'Image crop size (image_size x image_size)')\n", 
        "Flags.DEFINE_float('max_delta', 0.4, 'max delta for brightness, contrast and hue [0,0.5]')\n", 
        "Flags.DEFINE_float('max_saturation_delta', 2, 'max delta for saturation [0,3]')\n", 
        "\n", 
        "# model configurations\n", 
        "Flags.DEFINE_integer('embedding_size', 128, 'output embedding size')\n", 
        "Flags.DEFINE_float('dropout_rate', 0.1, 'Percentage of neuron to drop')\n", 
        "Flags.DEFINE_string('loss', 'hard', 'primary loss function. (semi-hard: triplet loss with semi-hard negative '\n", 
        "                                    'mining | hard: triplet loss with hard negative mining)')\n", 
        "Flags.DEFINE_float('loss_margin', 0.5, 'The learning rate for the network')\n", 
        "\n", 
        "# Trainer Parameters\n", 
        "Flags.DEFINE_float('learning_rate', 0.0001, 'The learning rate for the network')\n", 
        "Flags.DEFINE_integer('decay_step', 500000, 'The steps needed to decay the learning rate')\n", 
        "Flags.DEFINE_float('decay_rate', 0.1, 'The decay rate of each decay step')\n", 
        "Flags.DEFINE_boolean('stair', False, 'Whether perform staircase decay. True => decay in discrete interval.')\n", 
        "Flags.DEFINE_float('beta', 0.9, 'The beta1 parameter for the Adam optimizer')\n", 
        "Flags.DEFINE_integer('max_iter', 210000, 'The max iteration of the training')\n", 
        "Flags.DEFINE_integer('display_freq', 20, 'The diplay frequency of the training process')\n", 
        "Flags.DEFINE_integer('summary_freq', 100, 'The frequency of writing summary')\n", 
        "Flags.DEFINE_integer('save_freq', 1000, 'The frequency of saving checkpoint')\n", 
        "\n", 
        "FLAGS = Flags.FLAGS\n", 
        "\n", 
        "# Print the configuration of the model\n", 
        "print_configuration_op(FLAGS)\n", 
        "\n", 
        "# Check Directories\n", 
        "if FLAGS.output_dir is None or FLAGS.summary_dir is None:\n", 
        "    raise ValueError('The output directory and summary directory are needed')\n", 
        "\n", 
        "if FLAGS.train_dir is None or FLAGS.val_dir is None:\n", 
        "    raise ValueError('The train directory and val directory are needed')\n", 
        "\n", 
        "if not os.path.exists(FLAGS.train_dir) or not os.path.exists(FLAGS.val_dir):\n", 
        "    raise ValueError('The train directory and val directory should exist')\n", 
        "\n", 
        "# Check the output directory to save the checkpoint\n", 
        "if not os.path.exists(FLAGS.output_dir):\n", 
        "    os.mkdir(FLAGS.output_dir)\n", 
        "\n", 
        "# Check the summary directory to save the event\n", 
        "if not os.path.exists(FLAGS.summary_dir):\n", 
        "    os.mkdir(FLAGS.summary_dir)\n", 
        "\n", 
        "# Initialize DataLoader\n", 
        "data_loader = DataLoader(FLAGS)\n", 
        "data_size = data_loader.get_data_size()\n", 
        "print('[DATA LOADED] train size: %d with %d writers, val size: %d with %d writers' % (\n", 
        "    data_size.train, data_size.train_labels, data_size.val, data_size.val_labels))\n", 
        "\n", 
        "# Defining Placeholder\n", 
        "images_path_tensor = tf.placeholder(tf.string, shape=[None, ], name='image_path_tensors')\n", 
        "images_label_tensor = tf.placeholder(tf.int32, shape=[None, ], name='image_lables_tensor')\n", 
        "images_path_tensor_val = tf.placeholder(tf.string, shape=[None, ], name='images_path_tensor_val')\n", 
        "# # A hack to add validation accuracy in tensorboard\n", 
        "val_accuracy = tf.placeholder(tf.double, shape=[], name='val_accuracy')\n", 
        "\n", 
        "# Training\n", 
        "print('[INFO]: getting training model')\n", 
        "net = Network(FLAGS)\n", 
        "images_tensor = pre_process(images_path_tensor, FLAGS)\n", 
        "_print_shape = tf.Print(images_tensor, [tf.shape(images_tensor)], message=\"[INFO] current train batch shape: \",\n", 
        "                        first_n=1)\n", 
        "with tf.control_dependencies([_print_shape]):\n", 
        "    train = net(images_tensor, images_label_tensor)\n", 
        "\n", 
        "# Validation\n", 
        "val_image_tensor = pre_process(images_path_tensor_val, FLAGS, mode='val')\n", 
        "_print_val_shape = tf.Print(val_image_tensor, [tf.shape(val_image_tensor)], message=\"[INFO] current val batch shape: \",\n", 
        "                            first_n=1)\n", 
        "with tf.control_dependencies([_print_val_shape]):\n", 
        "    val_forward_pass = net.forward_pass(val_image_tensor)\n", 
        "\n", 
        "# Add summaries\n", 
        "print('[INFO]: Adding summaries')\n", 
        "tf.summary.histogram(\"embeddings_histogram\", train.embeddings)\n", 
        "tf.summary.image(\"train_images\", images_tensor, max_outputs=10)\n", 
        "tf.summary.scalar(\"train_loss\", train.loss)\n", 
        "tf.summary.scalar(\"l2_loss\", train.l2_loss)\n", 
        "tf.summary.scalar(\"triplet_loss\", train.triplet_loss)\n", 
        "tf.summary.scalar(\"learning_rate\", net.learning_rate)\n", 
        "tf.summary.scalar(\"val_accuracy\", val_accuracy)\n", 
        "\n", 
        "# Define the saver and weight initiallizer\n", 
        "saver = tf.train.Saver(max_to_keep=10)\n", 
        "\n", 
        "# Get trainable variable\n", 
        "train_var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"network\")\n", 
        "weight_initializer = tf.train.Saver(train_var_list)\n", 
        "\n", 
        "# Start the session\n", 
        "config = tf.ConfigProto()\n", 
        "config.gpu_options.allow_growth = True\n", 
        "\n", 
        "# Use supervisor to coordinate all queue and summary writer\n", 
        "# TODO: Deprecated, Update with tf.train.MonitoredTrainingSession\n", 
        "sv = tf.train.Supervisor(logdir=FLAGS.summary_dir, save_summaries_secs=0, saver=None)\n", 
        "\n", 
        "with sv.managed_session(config=config) as sess:\n", 
        "    # TODO: check the saving checkpoint part for below both\n", 
        "    if (FLAGS.checkpoint is not None) and (FLAGS.pre_trained_model is False):\n", 
        "        print('[INFO]: Loading model from the checkpoint...')\n", 
        "        checkpoint = tf.train.latest_checkpoint(FLAGS.checkpoint)\n", 
        "        saver.restore(sess, checkpoint)\n", 
        "\n", 
        "    elif (FLAGS.checkpoint is not None) and (FLAGS.pre_trained_model is True):\n", 
        "        print('[INFO]: Loading weights from the pre-trained model')\n", 
        "        weight_initializer.restore(sess, FLAGS.checkpoint)\n", 
        "\n", 
        "    print('[INFO] Optimization starts!!!')\n", 
        "    start = time.time()\n", 
        "    val_acc = 0\n", 
        "\n", 
        "    for step in range(FLAGS.max_iter):\n", 
        "\n", 
        "        batch = data_loader.get_train_batch()\n", 
        "        images_path, images_label = batch.images_path, batch.labels\n", 
        "\n", 
        "        # Validation\n", 
        "        # TODO: add validation images to tensorboard\n", 
        "        if ((step + 1) % FLAGS.display_freq) == 0 or ((step + 1) % FLAGS.summary_freq) == 0:\n", 
        "            # print(\"[INFO]: Validation Step.\")\n", 
        "            val_enroll_dict = data_loader.get_val_enrollment_batch().val_enroll_dict\n", 
        "            validation_batch_dict = data_loader.get_val_batch()\n", 
        "            val_acc = validate(sess, val_forward_pass, images_path_tensor_val, val_enroll_dict, validation_batch_dict,\n", 
        "                               FLAGS)\n", 
        "\n", 
        "        fetches = {\n", 
        "            \"train\": train.train,\n", 
        "            \"global_step\": net.global_step,\n", 
        "        }\n", 
        "\n", 
        "        if ((step + 1) % FLAGS.display_freq) == 0:\n", 
        "            fetches[\"training_loss\"] = train.loss\n", 
        "            fetches[\"l2_loss\"] = train.l2_loss\n", 
        "            fetches[\"learning_rate\"] = net.learning_rate\n", 
        "\n", 
        "        if ((step + 1) % FLAGS.summary_freq) == 0:\n", 
        "            fetches[\"summary\"] = sv.summary_op\n", 
        "\n", 
        "        results = sess.run(fetches, feed_dict={images_path_tensor: images_path, images_label_tensor: images_label,\n", 
        "                                               val_accuracy: val_acc})\n", 
        "\n", 
        "        if ((step + 1) % FLAGS.summary_freq) == 0:\n", 
        "            print('[INFO]: Recording summary !!!!')\n", 
        "            sv.summary_writer.add_summary(results['summary'], results['global_step'])\n", 
        "\n", 
        "        if ((step + 1) % FLAGS.display_freq) == 0:\n", 
        "            print(\n", 
        "                \"[PROGRESS]: global step: %d | learning rate: %f | training_loss: %f | l2_loss: %f |val_accuracy %f\" % (\n", 
        "                    results['global_step'], results['learning_rate'], results['training_loss'], results['l2_loss'],\n", 
        "                    val_acc))\n", 
        "\n", 
        "        if ((step + 1) % FLAGS.save_freq) == 0:\n", 
        "            print('[INFO]: Save the checkpoint !!!!')\n", 
        "            # TODO: Check wehter result['global_step'] needs to be passed instead\n", 
        "            saver.save(sess, os.path.join(FLAGS.output_dir, 'model'), global_step=net.global_step)\n", 
        "\n", 
        "    print('[INFO]: Optimization done!!!!!!!!!!!!')\n"
      ], 
      "cell_type": "code", 
      "execution_count": null, 
      "outputs": [], 
      "metadata": {}
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3", 
      "name": "python3", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "3.6.1", 
      "pygments_lexer": "ipython3", 
      "codemirror_mode": {
        "version": 3, 
        "name": "ipython"
      }
    }, 
    "anaconda-cloud": {}
  }
}