{
  "nbformat_minor": 1, 
  "nbformat": 4, 
  "cells": [
    {
      "source": [
        "from __future__ import division, absolute_import, print_function\n", 
        "\n", 
        "import collections\n", 
        "import functools\n", 
        "\n", 
        "import tensorflow as tf\n", 
        "\n", 
        "from triplet_loss import batch_hard_triplet_loss\n", 
        "\n", 
        "# layers = tf.layers\n", 
        "layers = tf.keras.layers\n", 
        "\n", 
        "\n", 
        "# Loss Helper Functions\n", 
        "\n", 
        "def semihard_mining_triplet_loss(labels, embeddings, margin=1.0):\n", 
        "    return tf.contrib.losses.metric_learning.triplet_semihard_loss(labels, embeddings, margin=margin)\n", 
        "\n", 
        "\n", 
        "def hard_mining_triplet_loss(labels, embeddings, margin=1.0):\n", 
        "    return None  # TODO: Implement\n", 
        "\n", 
        "\n", 
        "# Model Helper Classes and Functions\n", 
        "\n", 
        "class ConvBlock(tf.keras.Model):\n", 
        "    def __init__(self, filters, stage, block, regularizer, drop_rate=0.1, kernel=3, strides=(2, 2)):\n", 
        "        super(ConvBlock, self).__init__(name='')\n", 
        "        filters1, filters2, filters3 = filters\n", 
        "\n", 
        "        conv_name_base = 'res' + str(stage) + block + '_branch'\n", 
        "        # bn_name_base = 'bn' + str(stage) + block + '_branch'\n", 
        "        do_name_base = 'do' + str(stage) + block + '_branch'\n", 
        "\n", 
        "        self.conv2a = layers.Conv2D(filters1, kernel_size=(1, 1), strides=strides, name=conv_name_base + '2a',\n", 
        "                                    kernel_regularizer=regularizer, bias_regularizer=regularizer)\n", 
        "        # self.bn2a = layers.BatchNormalization(name=bn_name_base + '2a')\n", 
        "        self.do2a = layers.Dropout(drop_rate, name=do_name_base + '2a')\n", 
        "\n", 
        "        self.conv2b = layers.Conv2D(filters2, kernel_size=kernel, padding='same', name=conv_name_base + '2b',\n", 
        "                                    kernel_regularizer=regularizer, bias_regularizer=regularizer)\n", 
        "        # self.bn2b = layers.BatchNormalization(name=bn_name_base + '2b')\n", 
        "        self.do2b = layers.Dropout(drop_rate, name=do_name_base + '2b')\n", 
        "\n", 
        "        self.conv2c = layers.Conv2D(filters3, kernel_size=(1, 1), name=conv_name_base + '2c',\n", 
        "                                    kernel_regularizer=regularizer, bias_regularizer=regularizer)\n", 
        "        # self.bn2c = layers.BatchNormalization(name=bn_name_base + '2c')\n", 
        "\n", 
        "        self.conv_shortcut = layers.Conv2D(filters3, kernel_size=(1, 1), strides=strides, name=conv_name_base + '1',\n", 
        "                                           kernel_regularizer=regularizer, bias_regularizer=regularizer)\n", 
        "        # self.bn_shortcut = layers.BatchNormalization(name=bn_name_base + '1')\n", 
        "\n", 
        "    def call(self, input_tensor, training=False, mask=None):\n", 
        "        x = self.conv2a(input_tensor)\n", 
        "        # x = self.bn2a(x, training=training)\n", 
        "        x = tf.nn.relu(x)\n", 
        "        x = self.do2a(x, training=training)\n", 
        "\n", 
        "        x = self.conv2b(x)\n", 
        "        # x = self.bn2b(x, training=training)\n", 
        "        x = tf.nn.relu(x)\n", 
        "        x = self.do2b(x, training=training)\n", 
        "\n", 
        "        x = self.conv2c(x)\n", 
        "        # x = self.bn2c(x, training=training)\n", 
        "\n", 
        "        shortcut = self.conv_shortcut(input_tensor)\n", 
        "        # shortcut = self.bn_shortcut(shortcut, training=training)\n", 
        "\n", 
        "        x += shortcut\n", 
        "        return tf.nn.relu(x)\n", 
        "\n", 
        "\n", 
        "class IdentityBlock(tf.keras.Model):\n", 
        "\n", 
        "    def __init__(self, filters, stage, block, regularizer, drop_rate=0.1, kernel_size=3):\n", 
        "        super(IdentityBlock, self).__init__(name='')\n", 
        "        filters1, filters2, filters3 = filters\n", 
        "\n", 
        "        conv_name_base = 'res' + str(stage) + block + '_branch'\n", 
        "        # bn_name_base = 'bn' + str(stage) + block + '_branch'\n", 
        "        do_name_base = 'do' + str(stage) + block + '_branch'\n", 
        "\n", 
        "        self.conv2a = layers.Conv2D(filters1, (1, 1), name=conv_name_base + '2a', kernel_regularizer=regularizer,\n", 
        "                                    bias_regularizer=regularizer)\n", 
        "        # self.bn2a = layers.BatchNormalization(name=bn_name_base + '2a')\n", 
        "        self.do2a = layers.Dropout(drop_rate, name=do_name_base + '2a')\n", 
        "\n", 
        "        self.conv2b = layers.Conv2D(filters2, kernel_size, padding='same', name=conv_name_base + '2b',\n", 
        "                                    kernel_regularizer=regularizer, bias_regularizer=regularizer)\n", 
        "        # self.bn2b = layers.BatchNormalization(name=bn_name_base + '2b')\n", 
        "        self.do2b = layers.Dropout(drop_rate, name=do_name_base + '2b')\n", 
        "\n", 
        "        self.conv2c = layers.Conv2D(filters3, (1, 1), name=conv_name_base + '2c', kernel_regularizer=regularizer,\n", 
        "                                    bias_regularizer=regularizer)\n", 
        "        # self.bn2c = layers.BatchNormalization(name=bn_name_base + '2c')\n", 
        "\n", 
        "    def call(self, input_tensor, training=False, mask=None):\n", 
        "        x = self.conv2a(input_tensor)\n", 
        "        # x = self.bn2a(x, training=training)\n", 
        "        x = tf.nn.relu(x)\n", 
        "        x = self.do2a(x, training=training)\n", 
        "\n", 
        "        x = self.conv2b(x)\n", 
        "        # x = self.bn2b(x, training=training)\n", 
        "        x = tf.nn.relu(x)\n", 
        "        x = self.do2b(x, training=training)\n", 
        "\n", 
        "        x = self.conv2c(x)\n", 
        "        # x = self.bn2c(x, training=training)\n", 
        "\n", 
        "        x += input_tensor\n", 
        "        return tf.nn.relu(x)\n", 
        "\n", 
        "\n", 
        "# Model\n", 
        "# TODO: Add variable scope if it doesnt work\n", 
        "# TODO: Add dropout and regularization\n", 
        "class Resnet50(tf.keras.Model):\n", 
        "\n", 
        "    def __init__(self, emb_size, drop_rate, regularizer):\n", 
        "        super(Resnet50, self).__init__(name='')\n", 
        "\n", 
        "        self.conv1 = layers.Conv2D(64, (7, 7), strides=(2, 2), padding='same', name='conv1'\n", 
        "                                   , kernel_regularizer=regularizer, bias_regularizer=regularizer)\n", 
        "        # self.bn_conv1 = layers.BatchNormalization(name='bn_conv1')\n", 
        "        self.max_pool = layers.MaxPooling2D((3, 3), strides=(2, 2), name='mx_pool1')\n", 
        "\n", 
        "        self.l2a = ConvBlock([64, 64, 256], stage=2, block='a', drop_rate=drop_rate, regularizer=regularizer,\n", 
        "                             strides=(1, 1))\n", 
        "        self.l2b = IdentityBlock([64, 64, 256], stage=2, drop_rate=drop_rate, regularizer=regularizer, block='b')\n", 
        "        self.l2c = IdentityBlock([64, 64, 256], stage=2, drop_rate=drop_rate, regularizer=regularizer, block='c')\n", 
        "\n", 
        "        self.l3a = ConvBlock([128, 128, 512], stage=3, drop_rate=drop_rate, regularizer=regularizer, block='a')\n", 
        "        self.l3b = IdentityBlock([128, 128, 512], stage=3, drop_rate=drop_rate, regularizer=regularizer, block='b')\n", 
        "        self.l3c = IdentityBlock([128, 128, 512], stage=3, drop_rate=drop_rate, regularizer=regularizer, block='c')\n", 
        "        self.l3d = IdentityBlock([128, 128, 512], stage=3, drop_rate=drop_rate, regularizer=regularizer, block='d')\n", 
        "\n", 
        "        self.l4a = ConvBlock([256, 256, 1024], stage=4, drop_rate=drop_rate, regularizer=regularizer, block='a')\n", 
        "        self.l4b = IdentityBlock([256, 256, 1024], stage=4, drop_rate=drop_rate, regularizer=regularizer, block='b')\n", 
        "        self.l4c = IdentityBlock([256, 256, 1024], stage=4, drop_rate=drop_rate, regularizer=regularizer, block='c')\n", 
        "        self.l4d = IdentityBlock([256, 256, 1024], stage=4, drop_rate=drop_rate, regularizer=regularizer, block='d')\n", 
        "        self.l4e = IdentityBlock([256, 256, 1024], stage=4, drop_rate=drop_rate, regularizer=regularizer, block='e')\n", 
        "        self.l4f = IdentityBlock([256, 256, 1024], stage=4, drop_rate=drop_rate, regularizer=regularizer, block='f')\n", 
        "\n", 
        "        self.l5a = ConvBlock([512, 512, 2048], stage=5, drop_rate=drop_rate, regularizer=regularizer, block='a')\n", 
        "        self.l5b = IdentityBlock([512, 512, 2048], stage=5, drop_rate=drop_rate, regularizer=regularizer, block='b')\n", 
        "        self.l5c = IdentityBlock([512, 512, 2048], stage=5, drop_rate=drop_rate, regularizer=regularizer, block='c')\n", 
        "\n", 
        "        self.avg_pool = layers.AveragePooling2D((7, 7), strides=(7, 7), name='avg_pool1')\n", 
        "\n", 
        "        self.flatten = layers.Flatten()\n", 
        "        self.fc = layers.Dense(emb_size, name='fc1', kernel_regularizer=regularizer, bias_regularizer=regularizer)\n", 
        "\n", 
        "    def call(self, input_tensor, training=True, mask=None):\n", 
        "        x = self.conv1(input_tensor)\n", 
        "        # x = self.bn_conv1(x, training=training)\n", 
        "        x = tf.nn.relu(x)\n", 
        "        x = self.max_pool(x)\n", 
        "\n", 
        "        x = self.l2a(x, training=training)\n", 
        "        x = self.l2b(x, training=training)\n", 
        "        x = self.l2c(x, training=training)\n", 
        "\n", 
        "        x = self.l3a(x, training=training)\n", 
        "        x = self.l3b(x, training=training)\n", 
        "        x = self.l3c(x, training=training)\n", 
        "        x = self.l3d(x, training=training)\n", 
        "\n", 
        "        x = self.l4a(x, training=training)\n", 
        "        x = self.l4b(x, training=training)\n", 
        "        x = self.l4c(x, training=training)\n", 
        "        x = self.l4d(x, training=training)\n", 
        "        x = self.l4e(x, training=training)\n", 
        "        x = self.l4f(x, training=training)\n", 
        "\n", 
        "        x = self.l5a(x, training=training)\n", 
        "        x = self.l5b(x, training=training)\n", 
        "        x = self.l5c(x, training=training)\n", 
        "\n", 
        "        x = self.avg_pool(x)\n", 
        "        x = self.flatten(x)\n", 
        "        x = self.fc(x)\n", 
        "\n", 
        "        return x\n", 
        "\n", 
        "\n", 
        "# Network and Training op\n", 
        "# TODO: Check wether placeholder needed, Take care of private structure, make properties\n", 
        "class Network:\n", 
        "\n", 
        "    def __init__(self, FLAGS, reuse=False, var_scope='network'):\n", 
        "        self.FLAGS = FLAGS\n", 
        "        self.embedding_size = FLAGS.embedding_size\n", 
        "        self.drop_rate = FLAGS.dropout_rate\n", 
        "        self.var_scope = var_scope\n", 
        "        self.global_step = tf.train.get_or_create_global_step()\n", 
        "        self.learning_rate = FLAGS.learning_rate\n", 
        "        self.var_scope = var_scope\n", 
        "        self.reuse = reuse\n", 
        "        self.regularizer = tf.contrib.layers.l2_regularizer(scale=0.1)\n", 
        "\n", 
        "        self.net = Resnet50(self.embedding_size, self.drop_rate,\n", 
        "                            self.regularizer)\n", 
        "\n", 
        "        if FLAGS.loss == 'semi-hard':\n", 
        "            self.loss_fn = functools.partial(semihard_mining_triplet_loss, margin=FLAGS.loss_margin)\n", 
        "        elif FLAGS.loss == 'hard':\n", 
        "            self.loss_fn = functools.partial(batch_hard_triplet_loss, margin=FLAGS.loss_margin)\n", 
        "            # raise ValueError(\"loss fn not implemented: \" + FLAGS.loss)\n", 
        "        else:\n", 
        "            raise ValueError(\"unknown loss fn: \" + FLAGS.loss)\n", 
        "\n", 
        "    def __call__(self, inputs, labels, training=True):\n", 
        "        net_output = collections.namedtuple('net_output', 'embeddings, loss, l2_loss, triplet_loss, train')\n", 
        "        with tf.variable_scope(self.var_scope, reuse=self.reuse):\n", 
        "            embeddings = self.net(inputs, training=training)\n", 
        "        # NOTE: tf.losses.get_regularization_loss() doesnt work with tf.keras.layers,\n", 
        "        # So here l2 loss will always be zero.\n", 
        "        # If need to use regularization loss use tf.layers.\n", 
        "        l2_loss = tf.losses.get_regularization_loss()\n", 
        "        triplet_loss = self.loss_fn(labels=labels, embeddings=embeddings)\n", 
        "        loss = triplet_loss + l2_loss\n", 
        "\n", 
        "        with tf.variable_scope(\"optimizer\"):\n", 
        "            self.learning_rate = tf.train.exponential_decay(self.FLAGS.learning_rate, self.global_step,\n", 
        "                                                            self.FLAGS.decay_step,\n", 
        "                                                            self.FLAGS.decay_rate,\n", 
        "                                                            staircase=self.FLAGS.stair)\n", 
        "            incr_global_step = tf.assign(self.global_step, self.global_step + 1)\n", 
        "\n", 
        "            with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n", 
        "                tvars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.var_scope)\n", 
        "                optimizer = tf.train.AdamOptimizer(self.learning_rate, beta1=self.FLAGS.beta)\n", 
        "                grads_and_vars = optimizer.compute_gradients(loss, tvars)\n", 
        "                train_op = optimizer.apply_gradients(grads_and_vars)\n", 
        "\n", 
        "        # TODO: Add regularization loss\n", 
        "        return net_output(\n", 
        "            embeddings=embeddings,\n", 
        "            loss=loss,\n", 
        "            l2_loss=l2_loss,\n", 
        "            triplet_loss=triplet_loss,\n", 
        "            train=tf.group(loss, incr_global_step, train_op)\n", 
        "        )\n", 
        "\n", 
        "    def forward_pass(self, inputs):\n", 
        "        with tf.variable_scope(self.var_scope, reuse=tf.AUTO_REUSE):\n", 
        "            output = self.net(inputs, training=False)\n", 
        "        return output\n"
      ], 
      "cell_type": "code", 
      "execution_count": null, 
      "outputs": [], 
      "metadata": {}
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3", 
      "name": "python3", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "3.6.1", 
      "pygments_lexer": "ipython3", 
      "codemirror_mode": {
        "version": 3, 
        "name": "ipython"
      }
    }, 
    "anaconda-cloud": {}
  }
}